# Set which inference service to use, which model to use, and your API key
INFERENCE_SERVICE=groq  # supported are: groq, openrouter, openai, ollama, anthropic. DEFAULT anthropic
MODEL=llama3-8b-8192 # choose which model to use
API_KEY="" # set the API key of the provider you want to use
MAX_TOKENS=1000
# OLLAMA_URL=http://localhost:11434  # Only needed for Ollama
